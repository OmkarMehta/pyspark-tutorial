{"cells":[{"cell_type":"markdown","source":["# PySpark Read CSV file into DataFrame\n\n## &copy;  [Omkar Mehta](omehta2@illinois.edu) ##\n### Industrial and Enterprise Systems Engineering, The Grainger College of Engineering,  UIUC ###\n\n<hr style=\"border:2px solid blue\"> </hr>\n\nPySpark supports reading a CSV file with a pipe, comma, tab, space, or any other delimiter/separator files.\n\nPySpark provides csv(\"path\") on DataFrameReader to read a CSV file into PySpark DataFrame and dataframeObj.write.csv(\"path\") to save or write to the CSV file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0edd6124-6d93-496d-ac76-2cc766096be5"}}},{"cell_type":"code","source":["# PySpark Read CSV File into DataFrame\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType \nfrom pyspark.sql.types import ArrayType, DoubleType, BooleanType\nfrom pyspark.sql.functions import col,array_contains\n\nspark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate() #.builder().master(\"local[1]\")\ndf = spark.read.csv(\"/FileStore/tables/zipcodes.csv\")\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b655387-ce8e-4199-8431-a595553c2ac1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n |-- _c8: string (nullable = true)\n |-- _c9: string (nullable = true)\n |-- _c10: string (nullable = true)\n |-- _c11: string (nullable = true)\n |-- _c12: string (nullable = true)\n |-- _c13: string (nullable = true)\n |-- _c14: string (nullable = true)\n |-- _c15: string (nullable = true)\n |-- _c16: string (nullable = true)\n |-- _c17: string (nullable = true)\n |-- _c18: string (nullable = true)\n |-- _c19: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: string (nullable = true)\n-- _c1: string (nullable = true)\n-- _c2: string (nullable = true)\n-- _c3: string (nullable = true)\n-- _c4: string (nullable = true)\n-- _c5: string (nullable = true)\n-- _c6: string (nullable = true)\n-- _c7: string (nullable = true)\n-- _c8: string (nullable = true)\n-- _c9: string (nullable = true)\n-- _c10: string (nullable = true)\n-- _c11: string (nullable = true)\n-- _c12: string (nullable = true)\n-- _c13: string (nullable = true)\n-- _c14: string (nullable = true)\n-- _c15: string (nullable = true)\n-- _c16: string (nullable = true)\n-- _c17: string (nullable = true)\n-- _c18: string (nullable = true)\n-- _c19: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Using .format().load() way to read csv\ndf = spark.read.format(\"csv\").load(\"/FileStore/tables/zipcodes.csv\")\n\n#df = spark.read.format(\"org.apache.spark.sql.csv\").load(\"/FileStore/tables/zipcodes.csv\")\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef35f4f3-a13e-4209-ab7a-9423e8927f2f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n |-- _c8: string (nullable = true)\n |-- _c9: string (nullable = true)\n |-- _c10: string (nullable = true)\n |-- _c11: string (nullable = true)\n |-- _c12: string (nullable = true)\n |-- _c13: string (nullable = true)\n |-- _c14: string (nullable = true)\n |-- _c15: string (nullable = true)\n |-- _c16: string (nullable = true)\n |-- _c17: string (nullable = true)\n |-- _c18: string (nullable = true)\n |-- _c19: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _c0: string (nullable = true)\n-- _c1: string (nullable = true)\n-- _c2: string (nullable = true)\n-- _c3: string (nullable = true)\n-- _c4: string (nullable = true)\n-- _c5: string (nullable = true)\n-- _c6: string (nullable = true)\n-- _c7: string (nullable = true)\n-- _c8: string (nullable = true)\n-- _c9: string (nullable = true)\n-- _c10: string (nullable = true)\n-- _c11: string (nullable = true)\n-- _c12: string (nullable = true)\n-- _c13: string (nullable = true)\n-- _c14: string (nullable = true)\n-- _c15: string (nullable = true)\n-- _c16: string (nullable = true)\n-- _c17: string (nullable = true)\n-- _c18: string (nullable = true)\n-- _c19: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Using Header Record For Column Names\n\ndf2 = spark.read.option(\"header\",True).csv(\"/FileStore/tables/zipcodes.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36993696-6ca2-4445-80a1-3fb2c94c748d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Read Multiple CSV Files\n\n# df = spark.read.csv(\"path1,path2,path3\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a194ae04-a2a9-4cb5-84dd-3fa955391520"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Read all CSV Files in a Directory\ndf = spark.read.csv(\"/FileStore/tables/\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe3c19da-a47e-42a3-a4e0-e2f1687ac3a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Options While Reading CSV File\n\nYou can either use chaining option(self, key, value) to use multiple options or use alternate options(self, **options) method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53fba187-3ae5-493b-9cd6-f3f209a6ae54"}}},{"cell_type":"code","source":["# delimiter\ndf3 = spark.read.options(delimiter=',').csv(\"/FileStore/tables/zipcodes.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddc6bddb-69e8-46fb-883c-864532c366a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# infer schema\ndf4 = spark.read.options(inferSchema='True', delimiter=',').csv(\"/FileStore/tables/zipcodes.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf56714b-5c14-4278-beb8-66edb83612ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Alternatively, infer schema by\ndf4 = spark.read.option(\"inferSchema\",True) \\\n                .option(\"delimiter\",\",\") \\\n  .csv(\"/FileStore/tables/zipcodes.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51a413ea-3be9-4e78-bf65-ae84d1dfc741"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# header\ndf5 = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(\"/FileStore/tables/zipcodes.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ceabafb2-2f4e-478e-9e35-938ca0ac74f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### quotes\n\nWhen you have a column with a delimiter that used to split the columns, use `quotes` option to specify the quote character, by default it is ” and delimiters inside quotes are ignored. but using this option you can set any character.\n\n### nullValues\n\nUsing `nullValues` option you can specify the string in a CSV to consider as null. For example, if you want to consider a date column with a value `\"1900-01-01\"` set null on DataFrame.\n\n### dateFormat\n\n`dateFormat` option to used to set the format of the input DateType and TimestampType columns. Supports all java.text.SimpleDateFormat formats"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c516bf4-9554-4086-b8ab-1e17d192d14a"}}},{"cell_type":"code","source":["# Reading CSV files with a user-specified custom schema\nschema = StructType() \\\n      .add(\"RecordNumber\",IntegerType(),True) \\\n      .add(\"Zipcode\",IntegerType(),True) \\\n      .add(\"ZipCodeType\",StringType(),True) \\\n      .add(\"City\",StringType(),True) \\\n      .add(\"State\",StringType(),True) \\\n      .add(\"LocationType\",StringType(),True) \\\n      .add(\"Lat\",DoubleType(),True) \\\n      .add(\"Long\",DoubleType(),True) \\\n      .add(\"Xaxis\",IntegerType(),True) \\\n      .add(\"Yaxis\",DoubleType(),True) \\\n      .add(\"Zaxis\",DoubleType(),True) \\\n      .add(\"WorldRegion\",StringType(),True) \\\n      .add(\"Country\",StringType(),True) \\\n      .add(\"LocationText\",StringType(),True) \\\n      .add(\"Location\",StringType(),True) \\\n      .add(\"Decommisioned\",BooleanType(),True) \\\n      .add(\"TaxReturnsFiled\",StringType(),True) \\\n      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n      .add(\"TotalWages\",IntegerType(),True) \\\n      .add(\"Notes\",StringType(),True)\n      \ndf_with_schema = spark.read.format(\"csv\") \\\n      .option(\"header\", True) \\\n      .schema(schema) \\\n      .load(\"/FileStore/tables/zipcodes.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04cc3adf-abfa-44ad-b1e6-4d133b34fcb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Applying DataFrame transformations\n## Once you have created DataFrame from the CSV file, you can apply all transformation and actions DataFrame support.\n\n# Write PySpark DataFrame to CSV file\n\n# df.write.option(\"header\",True).csv(\"/tmp/spark_output/zipcodes\")\n\n# Options\n## Other options available quote,escape,nullValue,dateFormat,quoteMode .\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31470a3f-49ec-49f6-a833-dd84a12f88ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Saving modes - overwrite, append, ignore, error\n\n# df2.write.mode('overwrite').csv(\"/tmp/spark_output/zipcodes\")\n#//you can also use this\n# df2.write.format(\"csv\").mode('overwrite').save(\"/tmp/spark_output/zipcodes\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ca0d056-8caa-4093-9163-4c8e73ad8071"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd5ee57d-2693-407d-ac8c-17986d0dd8ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark_readcsv_dataframe-omkar.ipynb","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2282}},"nbformat":4,"nbformat_minor":0}
